{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b7cd70aa374f80ac31262cc284ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model with output_hidden_states set to True\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_num_tokens: int = 25,\n",
    "    top_k: int = 5,\n",
    "    layer: int = 8,\n",
    "    temperature: float = 1.0,\n",
    "    stop_token_ids: list = [],\n",
    "    stop_words: list = [],\n",
    "    eos_weight: float = 2.0,\n",
    "    enable_logging: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using a language model.\n",
    "\n",
    "    Args:\n",
    "        model (model): The language model.\n",
    "        tokenizer (model): The tokenizer corresponding to the model.\n",
    "        prompt (str): The initial text to start generation from.\n",
    "        num_tokens (int, optional): The number of tokens to generate. Defaults to 5.\n",
    "        top_k (int, optional): The number of top tokens to consider for sampling. Defaults to 5.\n",
    "        layer (int, optional): The layer of the model to use for generation. Defaults to 8.\n",
    "        temperature (float, optional): The temperature for softmax. Defaults to 1.0.\n",
    "        stop_token_ids (list, optional): List of token ids that will end generation if sampled. Defaults to [].\n",
    "        stop_words (list, optional): List of words that will end generation if sampled. Defaults to [].\n",
    "        eos_weight (float, optional): The weight to assign to the EOS token. Defaults to 2.0.\n",
    "        enable_logging (bool, optional): Enable logging for debugging. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    \n",
    "    # Get the EOS token ID\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    stop_token_ids.append(eos_token_id)\n",
    "    output_tokens = []\n",
    "    \n",
    "    # Generate num_tokens tokens\n",
    "    for _ in range(max_num_tokens):\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get the output of the specified layer\n",
    "        layer_output = outputs.hidden_states[layer]\n",
    "\n",
    "        # Pass the output through the final linear layer\n",
    "        logits = model.lm_head(layer_output)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        # Increase the weight of the EOS token\n",
    "        probabilities[0, -1, eos_token_id] *= eos_weight\n",
    "\n",
    "        # Get the probabilities of the top k tokens\n",
    "        top_k_probabilities, top_k_indices = torch.topk(probabilities[0, -1], top_k)\n",
    "\n",
    "        # Normalize the top k probabilities\n",
    "        top_k_probabilities /= torch.sum(top_k_probabilities)\n",
    "\n",
    "        # Sample from the top k probability distribution\n",
    "        sampled_token_id = top_k_indices[torch.multinomial(top_k_probabilities, 1)].item()\n",
    "        output_tokens.append(sampled_token_id)\n",
    "        \n",
    "        # Decode the token id back into text\n",
    "        sampled_token_text = tokenizer.decode([sampled_token_id])\n",
    "        sampled_token = {'input_ids': torch.tensor([[1, sampled_token_id]]), 'attention_mask': torch.tensor([[1, 1]])}\n",
    "\n",
    "        # If the sampled token is a stop token or stop word, return the generated text\n",
    "        if sampled_token_id in stop_token_ids or sampled_token_text in stop_words:\n",
    "            generated_text = tokenizer.decode(inputs['input_ids'][0])\n",
    "            pure_output = tokenizer.decode(output_tokens)\n",
    "            return {'text':generated_text,'output':pure_output,'prompt':prompt}\n",
    "\n",
    "        combined_input_ids = torch.cat((inputs['input_ids'].detach().cpu(), sampled_token['input_ids'][:,1:]), dim=-1)\n",
    "        combined_attention_mask = torch.cat((inputs['attention_mask'].detach().cpu(), sampled_token['attention_mask'][:,1:]), dim=-1)\n",
    "        \n",
    "        inputs = BatchEncoding({'input_ids': combined_input_ids, 'attention_mask': combined_attention_mask}).to(device)\n",
    "\n",
    "        # Log the token and top k tokens if logging is enabled\n",
    "        if enable_logging:\n",
    "            print(f'Token: {sampled_token_text}')\n",
    "            print('      -- Top tokens --')\n",
    "            for i in range(top_k):\n",
    "                token = tokenizer.decode([top_k_indices[i].item()])\n",
    "                probability = top_k_probabilities[i].item()\n",
    "                print(f'   {token}: {probability}')\n",
    "\n",
    "            print()\n",
    "    generated_text = tokenizer.decode(inputs['input_ids'][0])\n",
    "    pure_output = tokenizer.decode(output_tokens)\n",
    "    # return {'text':generated_text,'output':pure_output,'prompt':prompt}\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Prem\n",
      "      -- Top tokens --\n",
      "   Mand: 0.28617581725120544\n",
      "   Prem: 0.19170637428760529\n",
      "   ears: 0.19133175909519196\n",
      "   Het: 0.1694001704454422\n",
      "   gresql: 0.16138584911823273\n",
      "\n",
      "Token: Prem\n",
      "      -- Top tokens --\n",
      "   ゆ: 0.2643747329711914\n",
      "   Prem: 0.22167639434337616\n",
      "   ette: 0.1893041580915451\n",
      "   commission: 0.163179412484169\n",
      "   ља: 0.16146530210971832\n",
      "\n",
      "Token: Singh\n",
      "      -- Top tokens --\n",
      "   Spr: 0.30036622285842896\n",
      "   aj: 0.2109912782907486\n",
      "   hi: 0.17049676179885864\n",
      "   ette: 0.16448424756526947\n",
      "   Singh: 0.15366148948669434\n",
      "\n",
      "Token: hel\n",
      "      -- Top tokens --\n",
      "   hel: 0.5729809403419495\n",
      "   (: 0.13473165035247803\n",
      "   Dutch: 0.12364538013935089\n",
      "   op: 0.10515786707401276\n",
      "   : 0.06348417699337006\n",
      "\n",
      "Token: Son\n",
      "      -- Top tokens --\n",
      "   i: 0.8694928884506226\n",
      "   own: 0.0381438173353672\n",
      "   Giorg: 0.03432023897767067\n",
      "   rec: 0.0301945973187685\n",
      "   Son: 0.02784840390086174\n",
      "\n",
      "Token: ham\n",
      "      -- Top tokens --\n",
      "   ham: 0.33561453223228455\n",
      "   @@: 0.23042529821395874\n",
      "   rich: 0.16570591926574707\n",
      "   angol: 0.13532032072544098\n",
      "   ID: 0.13293391466140747\n",
      "\n",
      "Token: ana\n",
      "      -- Top tokens --\n",
      "   ana: 0.4590388238430023\n",
      "   ni: 0.15692278742790222\n",
      "   pton: 0.1456441879272461\n",
      "   ur: 0.12441007792949677\n",
      "   ça: 0.11398418992757797\n",
      "\n",
      "Token: ço\n",
      "      -- Top tokens --\n",
      "   ST: 0.28311750292778015\n",
      "   ço: 0.25279390811920166\n",
      "   cies: 0.1871935874223709\n",
      "   uchar: 0.16316628456115723\n",
      "   виси: 0.11372878402471542\n",
      "\n",
      "Token: \n",
      "      -- Top tokens --\n",
      "   imas: 0.21460753679275513\n",
      "   Û: 0.21444079279899597\n",
      "   Ć: 0.20813710987567902\n",
      "   : 0.19767744839191437\n",
      "   �: 0.16513711214065552\n",
      "\n",
      "Token: AJAX\n",
      "      -- Top tokens --\n",
      "   ️: 0.9493452906608582\n",
      "   р: 0.03406427800655365\n",
      "   AJAX: 0.0058443862944841385\n",
      "   ez: 0.005570650566369295\n",
      "   į: 0.00517537584528327\n",
      "\n",
      "layer 8: <s> My name is Prem Prem Singh hel Sonhamanaço AJAX\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My name is\"\n",
    "output_8 = generate_text(model, tokenizer, prompt, max_num_tokens = 10, layer = 8, temperature = 0.1, stop_words=['\\n'], enable_logging=True)\n",
    "print(f'layer 8: {output_8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Außer\n",
      "      -- Top tokens --\n",
      "   Außer: 0.7033793926239014\n",
      "   ̄: 0.09360671788454056\n",
      "   Bedeut: 0.09052415937185287\n",
      "   pur: 0.06295628100633621\n",
      "   Prem: 0.04953346773982048\n",
      "\n",
      "Token: gr\n",
      "      -- Top tokens --\n",
      "   gr: 0.4242660701274872\n",
      "   tie: 0.26104244589805603\n",
      "   p: 0.12375162541866302\n",
      "   poj: 0.12263013422489166\n",
      "   norm: 0.06830968707799911\n",
      "\n",
      "Token: péri\n",
      "      -- Top tokens --\n",
      "   péri: 0.8730854392051697\n",
      "   statunit: 0.040330689400434494\n",
      "   xxx: 0.032812707126140594\n",
      "   Orange: 0.031317807734012604\n",
      "   hab: 0.02245336025953293\n",
      "\n",
      "Token: Um\n",
      "      -- Top tokens --\n",
      "   amb: 0.28458425402641296\n",
      "   hat: 0.21934747695922852\n",
      "   odkazy: 0.19791728258132935\n",
      "   cia: 0.180925190448761\n",
      "   Um: 0.11722587794065475\n",
      "\n",
      "Token: idense\n",
      "      -- Top tokens --\n",
      "   idense: 0.3420819044113159\n",
      "   bre: 0.326993852853775\n",
      "   endi: 0.17018598318099976\n",
      "   ass: 0.08770638704299927\n",
      "   bers: 0.073031947016716\n",
      "\n",
      "Token: ur\n",
      "      -- Top tokens --\n",
      "   ur: 0.9688129425048828\n",
      "   ek: 0.02209775149822235\n",
      "   curity: 0.004918079357594252\n",
      "   ™: 0.0025291726924479008\n",
      "   i: 0.0016420643078163266\n",
      "\n",
      "Token: cles\n",
      "      -- Top tokens --\n",
      "   甲: 0.8871354460716248\n",
      "   face: 0.0526021271944046\n",
      "   ge: 0.029374705627560616\n",
      "   cles: 0.015547498129308224\n",
      "   icht: 0.015340302139520645\n",
      "\n",
      "Token: o\n",
      "      -- Top tokens --\n",
      "   que: 0.4608290195465088\n",
      "   hip: 0.3436584174633026\n",
      "   o: 0.1778406947851181\n",
      "   e: 0.009324505925178528\n",
      "   circum: 0.008347327820956707\n",
      "\n",
      "Token: ft\n",
      "      -- Top tokens --\n",
      "   fer: 0.8227035999298096\n",
      "   ft: 0.11297431588172913\n",
      "   usammen: 0.057657741010189056\n",
      "   ւ: 0.004532611463218927\n",
      "   aked: 0.002131690038368106\n",
      "\n",
      "Token: ™\n",
      "      -- Top tokens --\n",
      "   ™: 0.9355258345603943\n",
      "   itud: 0.05635548010468483\n",
      "   stack: 0.0033756550401449203\n",
      "   spot: 0.0025271254125982523\n",
      "   ®: 0.0022158955689519644\n",
      "\n",
      "layer 16: <s> My name is Außergr péri Umidenseurclesoft™\n"
     ]
    }
   ],
   "source": [
    "output_16 = generate_text(model, tokenizer, prompt, max_num_tokens = 10, layer = 16, temperature = 0.1, stop_words=['\\n'], enable_logging=True)\n",
    "print(f'layer 16: {output_16}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: in\n",
      "      -- Top tokens --\n",
      "   in: 0.9529890418052673\n",
      "   .: 0.046198099851608276\n",
      "   …: 0.0008117356919683516\n",
      "   today: 9.878916671368643e-07\n",
      "   and: 8.411416985154574e-08\n",
      "\n",
      "Token: bold\n",
      "      -- Top tokens --\n",
      "   bold: 0.9978393316268921\n",
      "   in: 0.0020528656896203756\n",
      "   capital: 6.939901504665613e-05\n",
      "   exp: 2.5015280698426068e-05\n",
      "   print: 1.3429616046778392e-05\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.5508857369422913\n",
      "   bold: 0.37772342562675476\n",
      "   font: 0.07139083743095398\n",
      "   Ital: 7.815165048441486e-08\n",
      "   letters: 3.5846399182304367e-09\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.9998922348022461\n",
      "   ics: 0.00010779927833937109\n",
      "   bold: 1.0148318851932459e-09\n",
      "   font: 2.600940574026822e-10\n",
      "   Ital: 3.535142664712332e-11\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.9992073178291321\n",
      "   Ital: 0.0007926259422674775\n",
      "   Ital: 3.913533497268418e-09\n",
      "   ics: 6.642618041990261e-14\n",
      "   bold: 2.3764170020918254e-14\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.9977964162826538\n",
      "   Ital: 0.002203636337071657\n",
      "   Ital: 1.7588640266463784e-10\n",
      "   bold: 4.06045314559067e-13\n",
      "   ital: 1.0842800995166518e-15\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.9679887890815735\n",
      "   Ital: 0.03201122581958771\n",
      "   Ital: 3.4799713111155484e-10\n",
      "   bold: 5.242234055700956e-13\n",
      "   ital: 5.465945960279363e-14\n",
      "\n",
      "Token: Ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.5717143416404724\n",
      "   Ital: 0.42828568816185\n",
      "   Ital: 9.104055009867551e-11\n",
      "   ital: 3.4269297469191606e-12\n",
      "   bold: 1.094485613251095e-12\n",
      "\n",
      "Token: ital\n",
      "      -- Top tokens --\n",
      "   ital: 0.8610267043113708\n",
      "   Ital: 0.08492794632911682\n",
      "   ics: 0.05404489114880562\n",
      "   .: 2.5516763457744673e-07\n",
      "   bold: 2.2812690758655663e-07\n",
      "\n",
      "Token: Ital\n",
      "      -- Top tokens --\n",
      "   Ital: 0.992312490940094\n",
      "   ital: 0.007687553763389587\n",
      "   Ital: 1.2706727336997403e-10\n",
      "   ital: 1.8849290184252965e-11\n",
      "   bold: 1.381961125396236e-11\n",
      "\n",
      "layer 24: <s> My name is in bold ital ital ital ital ital Ital ital Ital\n"
     ]
    }
   ],
   "source": [
    "output_24 = generate_text(model, tokenizer, prompt, max_num_tokens = 10, layer = 24, temperature = 0.1, stop_words=['\\n'], enable_logging=True)\n",
    "print(f'layer 24: {output_24}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: K\n",
      "      -- Top tokens --\n",
      "   K: 0.9287389516830444\n",
      "   J: 0.030214475467801094\n",
      "   L: 0.02070675790309906\n",
      "   D: 0.014593163505196571\n",
      "   A: 0.005746637936681509\n",
      "\n",
      "Token: atie\n",
      "      -- Top tokens --\n",
      "   atie: 0.9969188570976257\n",
      "   yle: 0.0026733395643532276\n",
      "   atherine: 0.0004066026012878865\n",
      "   ait: 7.016427048256446e-07\n",
      "   else: 4.5767313849864877e-07\n",
      "\n",
      "Token: and\n",
      "      -- Top tokens --\n",
      "   and: 0.9799004197120667\n",
      "   .: 0.018081780523061752\n",
      "   ,: 0.002017837017774582\n",
      "   K: 8.309279913437662e-13\n",
      "   H: 3.2420905695350333e-13\n",
      "\n",
      "Token: I\n",
      "      -- Top tokens --\n",
      "   I: 1.0\n",
      "   this: 1.7975648503336036e-16\n",
      "   my: 7.999980100584632e-19\n",
      "   i: 4.267395762701532e-23\n",
      "   in: 2.2425649066771086e-24\n",
      "\n",
      "Token: am\n",
      "      -- Top tokens --\n",
      "   am: 0.9986220598220825\n",
      "   ’: 0.0010929569834843278\n",
      "   ': 0.00028478875174187124\n",
      "   have: 1.4038313622677379e-07\n",
      "   live: 4.1338052980677276e-09\n",
      "\n",
      "Token: a\n",
      "      -- Top tokens --\n",
      "   a: 0.9999997615814209\n",
      "   the: 2.706961481635517e-07\n",
      "   : 5.0966402653784826e-08\n",
      "   an: 1.892337841979952e-08\n",
      "   from: 1.0973950914650032e-11\n",
      "\n",
      "Token: \n",
      "      -- Top tokens --\n",
      "   : 0.9999997615814209\n",
      "   wife: 1.8803005730205768e-07\n",
      "   senior: 6.670132535191442e-08\n",
      "   fre: 1.3724538838744138e-08\n",
      "   student: 7.033003956991024e-09\n",
      "\n",
      "Token: 2\n",
      "      -- Top tokens --\n",
      "   2: 0.9999907612800598\n",
      "   3: 9.128262718149927e-06\n",
      "   1: 1.3404400078798062e-07\n",
      "   4: 6.179589617794079e-10\n",
      "   5: 7.386737163779522e-14\n",
      "\n",
      "Token: 0\n",
      "      -- Top tokens --\n",
      "   0: 0.8495724201202393\n",
      "   2: 0.046468425542116165\n",
      "   3: 0.04568522050976753\n",
      "   5: 0.03440617397427559\n",
      "   4: 0.023867759853601456\n",
      "\n",
      "Token: year\n",
      "      -- Top tokens --\n",
      "   year: 0.9944604635238647\n",
      "   -: 0.005522304680198431\n",
      "   1: 1.7192724044434726e-05\n",
      "   0: 5.366409094875735e-09\n",
      "   something: 5.414544368420593e-10\n",
      "\n",
      "layer 32: <s> My name is Katie and I am a 20 year\n"
     ]
    }
   ],
   "source": [
    "output_32 = generate_text(model, tokenizer, prompt, max_num_tokens = 10, layer = 32, temperature = 0.1, stop_words=['\\n'], enable_logging=True)\n",
    "print(f'layer 32: {output_32}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: <s> My name is Prem Prem Singh hel Sonhamanaço AJAX, \n",
      "layer 16: <s> My name is Außergr péri Umidenseurclesoft™, \n",
      "layer 24: <s> My name is in bold ital ital ital ital ital Ital ital Ital, \n",
      "layer 32: <s> My name is Katie and I am a 20 year\n"
     ]
    }
   ],
   "source": [
    "print(f'layer 8: {output_8}, \\nlayer 16: {output_16}, \\nlayer 24: {output_24}, \\nlayer 32: {output_32}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
