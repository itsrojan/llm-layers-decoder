{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "whole_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_datasets = whole_dataset['train'].train_test_split(test_size=0.0001, seed=42)\n",
    "\n",
    "# Access the training and testing sets\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model path and the layers you want to evaluate\n",
    "model_name = 'Llama-2-7b-hf-fine-tuned'\n",
    "layers = [8, 16, 24, 32]\n",
    "\n",
    "# Load the model and tokenizer\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_num_tokens: int = 25,\n",
    "    top_k: int = 5,\n",
    "    layer: int = 8,\n",
    "    temperature: float = 1.0,\n",
    "    stop_token_ids: list = [],\n",
    "    stop_words: list = [],\n",
    "    eos_weight: float = 2.0,\n",
    "    enable_logging: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using a language model.\n",
    "\n",
    "    Args:\n",
    "        model (model): The language model.\n",
    "        tokenizer (model): The tokenizer corresponding to the model.\n",
    "        prompt (str): The initial text to start generation from.\n",
    "        num_tokens (int, optional): The number of tokens to generate. Defaults to 5.\n",
    "        top_k (int, optional): The number of top tokens to consider for sampling. Defaults to 5.\n",
    "        layer (int, optional): The layer of the model to use for generation. Defaults to 8.\n",
    "        temperature (float, optional): The temperature for softmax. Defaults to 1.0.\n",
    "        stop_token_ids (list, optional): List of token ids that will end generation if sampled. Defaults to [].\n",
    "        stop_words (list, optional): List of words that will end generation if sampled. Defaults to [].\n",
    "        eos_weight (float, optional): The weight to assign to the EOS token. Defaults to 2.0.\n",
    "        enable_logging (bool, optional): Enable logging for debugging. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    \n",
    "    # Get the EOS token ID\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    stop_token_ids.append(eos_token_id)\n",
    "    output_tokens = []\n",
    "    \n",
    "    # Generate num_tokens tokens\n",
    "    for _ in range(max_num_tokens):\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get the output of the specified layer\n",
    "        layer_output = outputs.hidden_states[layer]\n",
    "\n",
    "        # Pass the output through the final linear layer\n",
    "        logits = model.lm_head(layer_output)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        # Increase the weight of the EOS token\n",
    "        probabilities[0, -1, eos_token_id] *= eos_weight\n",
    "\n",
    "        # Get the probabilities of the top k tokens\n",
    "        top_k_probabilities, top_k_indices = torch.topk(probabilities[0, -1], top_k)\n",
    "\n",
    "        # Normalize the top k probabilities\n",
    "        top_k_probabilities /= torch.sum(top_k_probabilities)\n",
    "\n",
    "        # Sample from the top k probability distribution\n",
    "        sampled_token_id = top_k_indices[torch.multinomial(top_k_probabilities, 1)].item()\n",
    "        output_tokens.append(sampled_token_id)\n",
    "        \n",
    "        # Decode the token id back into text\n",
    "        sampled_token_text = tokenizer.decode([sampled_token_id])\n",
    "        sampled_token = {'input_ids': torch.tensor([[1, sampled_token_id]]), 'attention_mask': torch.tensor([[1, 1]])}\n",
    "\n",
    "        # If the sampled token is a stop token or stop word, return the generated text\n",
    "        if sampled_token_id in stop_token_ids or sampled_token_text in stop_words:\n",
    "            generated_text = tokenizer.decode(inputs['input_ids'][0])\n",
    "            pure_output = tokenizer.decode(output_tokens)\n",
    "            return {'text':generated_text,'output':pure_output,'prompt':prompt}\n",
    "\n",
    "        combined_input_ids = torch.cat((inputs['input_ids'].detach().cpu(), sampled_token['input_ids'][:,1:]), dim=-1)\n",
    "        combined_attention_mask = torch.cat((inputs['attention_mask'].detach().cpu(), sampled_token['attention_mask'][:,1:]), dim=-1)\n",
    "        \n",
    "        inputs = BatchEncoding({'input_ids': combined_input_ids, 'attention_mask': combined_attention_mask}).to(device)\n",
    "\n",
    "    generated_text = tokenizer.decode(inputs['input_ids'][0])\n",
    "    pure_output = tokenizer.decode(output_tokens)\n",
    "    # return {'text':generated_text,'output':pure_output,'prompt':prompt}\n",
    "    return pure_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What would be the best type of exercise for a person who has arthritis?\n",
      "\n",
      "### Input:\n",
      "No additional input provided.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: in order ofBeanFactorynaríoasaheb Joh (\n",
      "Layer 16: аmp href externasenior_rolaugustine\n",
      "Layer 24: gentle aer aer aer aer aer aer aer aer aer\n",
      "Layer 32: The best type of exercise for a person with ar\n",
      "Reference Answer: For someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 2: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Calculate the atomic mass for lithium.\n",
      "\n",
      "### Input:\n",
      "No additional input provided.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: in.eduuilanteuppertchiá aug\n",
      "Layer 16: The inoxerience ofﬁ️️️\n",
      "Layer 24: l Lithium atomic mass is mass number of atom\n",
      "Layer 32: The atomic mass for lithium is 6\n",
      "Reference Answer: The atomic mass for lithium is 6.941 u (units). This is determined from the number of protons and neutrons in the nucleus of a lithium atom, which is 3 protons and 4 neutrons. Each proton and neutron has a mass of 1.007 u, resulting in a total mass of 6.941 u.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 3: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the following binary code to ASCII characters.\n",
      "\n",
      "### Input:\n",
      "01110101 01100001 01110010 01101001 01110100\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: ogerastequevoạ�Flagship Traceback de\n",
      "Layer 16: Archivlink://www …ALSEuilobreрemeission\n",
      "Layer 24: {'text': '<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nConvert the following binary code to ASCII characters.\\n\\n### Input:\\n01110101 01100001 01110010 01101001 01110100\\n\\n### Response:\\n characters: Hello world', 'output': 'characters: Hello world\\n', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nConvert the following binary code to ASCII characters.\\n\\n### Input:\\n01110101 01100001 01110010 01101001 01110100\\n\\n### Response:\\n'}\n",
      "Layer 32: A, B, C, D, E,\n",
      "Reference Answer: The ASCII characters for the binary code is: wast.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 4: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Take this sentence and make it more descriptive: She was scared.\n",
      "\n",
      "### Input:\n",
      "She was scared.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: ченко,framiezovitypeneuwcontrolsdonnées autory\n",
      "Layer 16: аloc�️️️️️️️\n",
      "Layer 24: she terrorized by terrorized by terrorized by\n",
      "Layer 32: She was petrified, her heart pounding\n",
      "Reference Answer: She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 5: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Brainstorm 3 possible ideas to promote the following product\n",
      "\n",
      "### Input:\n",
      "New motorized skateboard\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: forego detection Bedeutlesetlosnesses Janu\n",
      "Layer 16: •    шиema ipsum dolorstagging socialmedia\n",
      "Layer 24: ideas to Promprom motorized sk sk sk sk\n",
      "Layer 32: 1. Host a skateboarding competition with\n",
      "Reference Answer: 1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 6: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using imperative mood.\n",
      "\n",
      "### Input:\n",
      "Please pick up the book from the shelf.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Layer 8: to coll virtuelmentescheragt Хронологија┌ IPv\n",
      "Layer 16: obeuntimelpatformerRORsmbHkeitenhape\n",
      "Layer 24: pick up the book from the shelf now.\n",
      "Layer 32: Pick up the book from the shelf.\n",
      "Reference Answer: Pick up the book from the shelf.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example function to print outputs for different layers using test dataset input and instructions as prompts\n",
    "# and incorporating a specific prompt structure\n",
    "def print_layer_outputs(test_dataset, model, tokenizer, layers, max_num_tokens=10, temperature=0.1, stop_words=['\\n']):\n",
    "    results = []  # List to store results for future evaluation\n",
    "\n",
    "    # Defining the introductory part of the prompt\n",
    "    directive_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "\n",
    "    for idx, example in enumerate(test_dataset):\n",
    "        # Creating a detailed prompt with clearly defined sections\n",
    "        if 'input' in example and example['input'].strip():  # Check if 'input' column exists and is not empty\n",
    "            detailed_prompt = f\"{directive_prompt}\\n\\n### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\n",
    "        else:\n",
    "            detailed_prompt = f\"{directive_prompt}\\n\\n### Instruction:\\n{example['instruction']}\\n\\n### Input:\\nNo additional input provided.\\n\\n### Response:\\n\"\n",
    "\n",
    "        # Printing the formatted prompt\n",
    "        print(f\"Prompt {idx+1}: {detailed_prompt}\")\n",
    "        \n",
    "        reference_answer = example.get('output', 'No reference answer provided.')  # Get reference answer\n",
    "        layer_outputs = {}\n",
    "\n",
    "        # Generating responses from each layer\n",
    "        for layer in layers:\n",
    "            output = generate_text(model, tokenizer, detailed_prompt, max_num_tokens=max_num_tokens, layer=layer, temperature=temperature, stop_words=stop_words)\n",
    "            print(f'Layer {layer}: {output}')\n",
    "            layer_outputs[f'Layer {layer}'] = output\n",
    "\n",
    "        # Save prompt, reference, and generated outputs for evaluation\n",
    "        results.append({\n",
    "            'prompt': detailed_prompt,\n",
    "            'reference_answer': reference_answer,\n",
    "            'generated_outputs': layer_outputs\n",
    "        })\n",
    "        print(f\"Reference Answer: {reference_answer}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Save results to a JSON file for future use\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Use this function to display the outputs\n",
    "print_layer_outputs(test_dataset, ft_model, ft_tokenizer, layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 17%|█▋        | 1/6 [00:08<00:44,  8.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 2/6 [00:17<00:35,  8.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Output for layer Layer 24 is not a string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 3/6 [00:24<00:23,  7.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 67%|██████▋   | 4/6 [00:33<00:16,  8.46s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 83%|████████▎ | 5/6 [00:43<00:08,  8.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 6/6 [00:52<00:00,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-wise Average Scores: {\n",
      "    \"Layer 8\": {\n",
      "        \"BLEU\": 0.0,\n",
      "        \"ROUGE-L\": 0.005847953216374269,\n",
      "        \"BERTScore\": 0.7701817949612936\n",
      "    },\n",
      "    \"Layer 16\": {\n",
      "        \"BLEU\": 2.131077415413766e-09,\n",
      "        \"ROUGE-L\": 0.011695906432748537,\n",
      "        \"BERTScore\": 0.7627338667710623\n",
      "    },\n",
      "    \"Layer 24\": {\n",
      "        \"BLEU\": 0.10354390070393855,\n",
      "        \"ROUGE-L\": 0.2345180023228804,\n",
      "        \"BERTScore\": 0.8339886784553527\n",
      "    },\n",
      "    \"Layer 32\": {\n",
      "        \"BLEU\": 0.1729871495321498,\n",
      "        \"ROUGE-L\": 0.3130216251010039,\n",
      "        \"BERTScore\": 0.8921485543251038\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_text_quality(reference, candidate):\n",
    "    def calculate_bleu(reference, candidate):\n",
    "        reference_tokens = [reference.split()]\n",
    "        candidate_tokens = candidate.split()\n",
    "        smoothie = SmoothingFunction().method1  # Experiment with different methods\n",
    "        return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "    def calculate_rouge_l(reference, candidate):\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        return scorer.score(reference, candidate)['rougeL'].fmeasure\n",
    "\n",
    "    def calculate_bert_score(reference, candidate):\n",
    "        *_, bert_scores = score([candidate], [reference], lang='en', return_hash=False)\n",
    "        return bert_scores.mean().item()\n",
    "\n",
    "    # Check that both reference and candidate are strings\n",
    "    if not isinstance(reference, str) or not isinstance(candidate, str):\n",
    "        print(\"Error: Non-string input to evaluate_text_quality. Reference or candidate is not a string.\")\n",
    "        return {'BLEU': 0, 'ROUGE-L': 0, 'BERTScore': 0}\n",
    "\n",
    "    return {\n",
    "        'BLEU': calculate_bleu(reference, candidate),\n",
    "        'ROUGE-L': calculate_rouge_l(reference, candidate),\n",
    "        'BERTScore': calculate_bert_score(reference, candidate)\n",
    "    }\n",
    "\n",
    "def calculate_scores(data):\n",
    "    scores_per_layer = {}\n",
    "    \n",
    "    for entry in tqdm(data):\n",
    "        reference_answer = entry['reference_answer']\n",
    "        generated_outputs = entry['generated_outputs']\n",
    "        \n",
    "        for layer, output in generated_outputs.items():\n",
    "            # Ensure output is a string\n",
    "            if not isinstance(output, str):\n",
    "                print(f\"Error: Output for layer {layer} is not a string.\")\n",
    "                continue\n",
    "            \n",
    "            evaluation_results = evaluate_text_quality(reference_answer, output)\n",
    "            \n",
    "            if layer not in scores_per_layer:\n",
    "                scores_per_layer[layer] = {'BLEU': 0, 'ROUGE-L': 0, 'BERTScore': 0, 'count': 0}\n",
    "            \n",
    "            for key in ['BLEU', 'ROUGE-L', 'BERTScore']:\n",
    "                scores_per_layer[layer][key] += evaluation_results[key]\n",
    "            \n",
    "            scores_per_layer[layer]['count'] += 1\n",
    "\n",
    "    # Average the scores\n",
    "    average_scores_per_layer = {}\n",
    "    for layer, scores in scores_per_layer.items():\n",
    "        if scores['count'] == 0:\n",
    "            continue\n",
    "        average_scores_per_layer[layer] = {key: scores[key] / scores['count'] for key in ['BLEU', 'ROUGE-L', 'BERTScore']}\n",
    "    \n",
    "    return average_scores_per_layer\n",
    "\n",
    "# Load results from JSON file\n",
    "with open('results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Calculate and print the scores\n",
    "scores_model = calculate_scores(data)\n",
    "print(\"Layer-wise Average Scores:\", json.dumps(scores_model, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
